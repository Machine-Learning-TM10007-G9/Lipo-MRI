{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment lipomas G9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "d94f2ab5-28a7-4762-ba29-535959eaa80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/Machine-Learning-TM10007-G9/Lipo-MRI.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z75tolmAqj3z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuSR7zcbNW0h"
      },
      "source": [
        "# **Data loading**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "39841b5b-69e7-4416-d248-554f580a9a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 115\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "from worclipo.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import important functions**"
      ],
      "metadata": {
        "id": "g8ivKj_yynDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "mSI_faQPNW0i"
      },
      "outputs": [],
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import decomposition\n",
        "from scipy import stats\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create X and Y**"
      ],
      "metadata": {
        "id": "eFLxHJ2Az70n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 1:].values\n",
        "y = np.array(data['label'])"
      ],
      "metadata": {
        "id": "DtPbcpXFz7au"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split the data**"
      ],
      "metadata": {
        "id": "JIfugNhPzDo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, stratify=y)\n",
        "\n",
        "#print(len(X_train))\n",
        "#print(len(X_test))\n",
        "#print(len(y_train))\n",
        "#print(len(y_test))"
      ],
      "metadata": {
        "id": "TmcPnIBdzCG_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checks if all the data points are floats**\n",
        "Is 0.0 a missing value?"
      ],
      "metadata": {
        "id": "eccXHmAeb1OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for row in X_train:\n",
        "  for col in row:\n",
        "    if not isinstance(col, float):\n",
        "        print(col)"
      ],
      "metadata": {
        "id": "4aVtVkv1dljh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check for missing values**\n",
        "\n"
      ],
      "metadata": {
        "id": "6Th7iN-wiDCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = np.sum(np.isnan(X_train))\n",
        "\n",
        "print(\"Number of missing values:\",missing_values)"
      ],
      "metadata": {
        "id": "xz_YEZ4niOB0",
        "outputId": "971a4889-c7bf-4c9f-ada0-9cbcd48e9307",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check for duplicates**\n",
        "Can we assume that duplicates values are duplicates?"
      ],
      "metadata": {
        "id": "OGrob7DIZl6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find duplicate columns and delete them\n",
        "def delete_duplicate_columns(arr):\n",
        "    num_cols = arr.shape[1]\n",
        "    duplicate_columns = []\n",
        "    for i in range(num_cols):\n",
        "        for j in range(i + 1, num_cols):\n",
        "            if np.array_equal(arr[:, i], arr[:, j]):\n",
        "                duplicate_columns.append(j)  # Append index of the duplicate column\n",
        "    if duplicate_columns:\n",
        "        # Delete duplicate columns\n",
        "        arr_without_duplicates = np.delete(arr, duplicate_columns, axis=1)\n",
        "        return arr_without_duplicates\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# Remove duplicate columns\n",
        "X_train_clean = delete_duplicate_columns(X_train)\n",
        "\n",
        "print(\"Number of columns in original array:\",X_train.shape[1])\n",
        "print(\"Number of columns after removal of duplicate columns:\",X_train_clean.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyEMNXijZ37p",
        "outputId": "e8b51df6-2da8-4639-ba1b-c5687b663ec6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns in original array: 493\n",
            "Number of columns after removal of duplicate columns: 465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking for outliers**\n",
        "Using the Z-score"
      ],
      "metadata": {
        "id": "6c9Rj_XInE6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows, num_cols = X_train_clean.shape\n",
        "total_outliers = 0\n",
        "\n",
        "for i in range(num_cols):\n",
        "    z = np.abs(stats.zscore(X_train_clean[:,i]))\n",
        "\n",
        "    threshold = 3\n",
        "    outliers = X_train_clean[z > threshold, i]\n",
        "    total_outliers +=(len(outliers))\n",
        "\n",
        "print(f'Total number of outliers  = {total_outliers}/{num_rows*num_cols}')\n",
        "print(f'Avarage number of outliers per feature = {round(total_outliers/num_cols,2)}/{num_rows}')\n"
      ],
      "metadata": {
        "id": "mPIPeNp-nJTv",
        "outputId": "d389ca1c-3e8a-42df-b32f-dfff0006da1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of outliers  = 651/42780\n",
            "Avarage number of outliers per feature = 1.4/92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check if the data is normally distributed**\n",
        "Using the Kolmogorov Smirnov"
      ],
      "metadata": {
        "id": "u3RmZ5isz4Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_not_normal = 0\n",
        "p_threshold = 0.05\n",
        "\n",
        "for l in range(num_cols):\n",
        "    kstest_result = stats.kstest(X_train_clean[:,l], 'norm')\n",
        "    if kstest_result.pvalue < p_threshold:\n",
        "        total_not_normal += 1\n",
        "\n",
        "print(f'Total features that are not normally distributed = {total_not_normal}/{num_cols}')\n"
      ],
      "metadata": {
        "id": "PB_LKu7Bz3Ul",
        "outputId": "bb472f0c-162c-4a34-8e22-5daeb3c5223b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features that are not normally distributed = 465/465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scaling**\n",
        "Robust scaling because outliers and non Gaussian distribution"
      ],
      "metadata": {
        "id": "Ammb7jQ7RCPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data to be normal\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "scaler.fit(X_train_clean)\n",
        "X_train_scaled = scaler.transform(X_train_clean)"
      ],
      "metadata": {
        "id": "SjdjlpnSRGGd"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Removing features with low variance**"
      ],
      "metadata": {
        "id": "GQB0juIpJIcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute variance along each feature (axis=0)\n",
        "variances = np.var(X_train_scaled, axis=0)\n",
        "\n",
        "# Define a threshold for variance\n",
        "threshold = 2\n",
        "\n",
        "# Filter out features with variance below the threshold\n",
        "high_variance_indices = np.where(variances >= threshold)[0]\n",
        "\n",
        "# Select only the high variance features from the dataset\n",
        "data_high_variance = X_train_scaled[:, high_variance_indices]\n",
        "\n",
        "# Display the selected features\n",
        "print(\"Selected features with high variance:\")\n",
        "print(data_high_variance.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDc0QBkoOEbV",
        "outputId": "5f55c85b-7e68-4cb2-b18e-3c4b1133ab57"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features with high variance:\n",
            "(92, 58)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature selection**\n",
        "Using L1 because it can better handle high demensionality, it migigates overfitting and is easy to interpret"
      ],
      "metadata": {
        "id": "qNAjbYOLjx53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"Original data shape:\",X_train_scaled.shape)\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "lsvc = LogisticRegression(C=1.9, penalty=\"l1\", dual=False, solver='liblinear').fit(X_train_scaled, y_train)\n",
        "model = SelectFromModel(lsvc, prefit=True)\n",
        "X_feature = model.transform(X_train_scaled)\n",
        "print(\"New data shape:\",X_feature.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqBnoaZ1SJHK",
        "outputId": "e16e26d4-7b84-46b6-9514-2939015942db"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (92, 465)\n",
            "New data shape: (92, 55)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA**"
      ],
      "metadata": {
        "id": "dm3msQI8WZ45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voer PCA uit\n",
        "pca = decomposition.PCA(n_components=10)  #n_components is the amount of features that remain after PCA (can also be a ratio)\n",
        "X_pca = pca.fit_transform(X_feature)\n",
        "\n",
        "# Verklaarde variantie ratio's\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Cumulatieve verklaarde variantie\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Maak een DataFrame om de resultaten mooi weer te geven\n",
        "df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
        "    'Explained Variance Ratio': explained_variance_ratio,\n",
        "    'Cumulative Explained Variance': cumulative_explained_variance\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qm-dD7rWg_g",
        "outputId": "36a86bfc-6915-4217-a716-fea3ebcc1133"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Principal Component  Explained Variance Ratio  Cumulative Explained Variance\n",
            "0                 PC1              9.999995e-01                       0.999999\n",
            "1                 PC2              4.713020e-07                       1.000000\n",
            "2                 PC3              5.717308e-08                       1.000000\n",
            "3                 PC4              1.051200e-08                       1.000000\n",
            "4                 PC5              8.603359e-09                       1.000000\n",
            "5                 PC6              6.244737e-10                       1.000000\n",
            "6                 PC7              5.060454e-10                       1.000000\n",
            "7                 PC8              2.118805e-10                       1.000000\n",
            "8                 PC9              1.665553e-10                       1.000000\n",
            "9                PC10              1.662022e-11                       1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-validation**"
      ],
      "metadata": {
        "id": "1AqAls9XifaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(X_train_clean, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_train_scaled[train_index], X_train_scaled[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "print(len(X_train_fold))\n",
        "print(len(X_val_fold))\n",
        "print(len(y_train_fold))\n",
        "print(len(y_val_fold))"
      ],
      "metadata": {
        "id": "CaWPtHtmimZS",
        "outputId": "c5669dfd-12d5-4e6d-d55e-9c35d985c979",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74\n",
            "18\n",
            "74\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random forest**"
      ],
      "metadata": {
        "id": "ansRTksVbive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "scores_rfc = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    rfc = RandomForestClassifier(n_estimators=100)\n",
        "    rfc.fit(X_train_fold,y_train_fold)\n",
        "    pred_rfc = rfc.predict(X_val_fold)\n",
        "    accuracy_rfc = accuracy_score(y_val_fold, pred_rfc)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_rfc.append(accuracy_rfc)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_rfc)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_rfc = sum(scores_rfc) / len(scores_rfc)\n",
        "print(\"Average Accuracy:\", average_accuracy_rfc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKeuhOtKbiFC",
        "outputId": "bbc62a23-805f-496b-8811-1cc61fd2bc19"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [0.8421052631578947, 0.7368421052631579, 0.7777777777777778, 0.8888888888888888, 0.7777777777777778]\n",
            "Average Accuracy: 0.8046783625730993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**"
      ],
      "metadata": {
        "id": "M57SpGqZeDhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "scores_clf = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    clf = svm.SVC()\n",
        "    clf.fit(X_train_fold,y_train_fold)\n",
        "    pred_clf = clf.predict(X_val_fold)\n",
        "    accuracy_clf = accuracy_score(y_val_fold, pred_clf)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_clf.append(accuracy_clf)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_clf)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_clf = sum(scores_clf) / len(scores_clf)\n",
        "print(\"Average Accuracy:\", average_accuracy_clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rygV2hV-eJ37",
        "outputId": "770c6fa9-c8d4-4f41-a087-bdd6bea84044"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [0.42105263157894735, 0.47368421052631576, 0.5, 0.5, 0.4444444444444444]\n",
            "Average Accuracy: 0.4678362573099415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural network**"
      ],
      "metadata": {
        "id": "SGCWQPk4fS1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "\n",
        "scores_mlpc = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in kf.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    mlpc = MLPClassifier(hidden_layer_sizes=(50,50),max_iter=500)\n",
        "    mlpc.fit(X_train_fold, y_train_fold)\n",
        "    pred_mlpc = mlpc.predict(X_val_fold)\n",
        "    accuracy_mlpc = accuracy_score(y_val_fold, pred_mlpc)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_mlpc.append(accuracy_mlpc)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_mlpc)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_mlpc = sum(scores_mlpc) / len(scores_mlpc)\n",
        "print(\"Average Accuracy:\", average_accuracy_mlpc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4dputYVfSc9",
        "outputId": "967218bb-00e8-43f4-e747-5e871051fd4c"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [0.6842105263157895, 0.5263157894736842, 0.3888888888888889, 0.6111111111111112, 0.6111111111111112]\n",
            "Average Accuracy: 0.5643274853801169\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
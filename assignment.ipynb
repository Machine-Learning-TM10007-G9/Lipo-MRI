{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SXpaKwwGe5x"
      },
      "source": [
        "# TM10007 Assignment lipomas G9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiDn2Sk-VWqE",
        "outputId": "68b25cc8-c536-4df3-ea9b-997a81cfcdc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/Machine-Learning-TM10007-G9/Lipo-MRI.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z75tolmAqj3z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuSR7zcbNW0h"
      },
      "source": [
        "# **Data loading**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NE_fTbKGe5z",
        "outputId": "b9385b85-3022-45b7-e153-99c7f43da137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 115\n",
            "The number of columns: 494\n"
          ]
        }
      ],
      "source": [
        "from worclipo.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import important functions**"
      ],
      "metadata": {
        "id": "g8ivKj_yynDm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mSI_faQPNW0i"
      },
      "outputs": [],
      "source": [
        "# General packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets as ds\n",
        "import seaborn\n",
        "\n",
        "\n",
        "# Classifiers\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import feature_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import neighbors\n",
        "from sklearn import svm\n",
        "from sklearn import decomposition\n",
        "from scipy import stats\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import LeaveOneOut"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create X and Y**"
      ],
      "metadata": {
        "id": "eFLxHJ2Az70n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, 1:].values\n",
        "y = np.array(data['label'])\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming y is a list of strings representing class labels\n",
        "class_counts = Counter(y)\n",
        "\n",
        "# Display class counts\n",
        "for class_label, count in class_counts.items():\n",
        "    print(f\"Class '{class_label}': {count} samples\")"
      ],
      "metadata": {
        "id": "DtPbcpXFz7au",
        "outputId": "1f440b56-dd5c-49af-936e-9557b5d34b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class 'liposarcoma': 58 samples\n",
            "Class 'lipoma': 57 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Split the data**"
      ],
      "metadata": {
        "id": "JIfugNhPzDo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=0)\n",
        "\n",
        "#print(len(X_train))\n",
        "#print(len(X_test))\n",
        "#print(len(y_train))\n",
        "#print(len(y_test))"
      ],
      "metadata": {
        "id": "TmcPnIBdzCG_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check for missing values**\n",
        "\n"
      ],
      "metadata": {
        "id": "6Th7iN-wiDCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = np.sum(np.isnan(X_train))\n",
        "\n",
        "print(\"Number of missing values:\",missing_values)"
      ],
      "metadata": {
        "id": "xz_YEZ4niOB0",
        "outputId": "bfebb3b3-ba6c-4c62-c2d2-e8e2f8e9e5c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of missing values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checks if all the data points are floats**"
      ],
      "metadata": {
        "id": "eccXHmAeb1OX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_floats = 0\n",
        "for row in X_train:\n",
        "  for col in row:\n",
        "    if not isinstance(col, float):\n",
        "        non_floats += 1\n",
        "\n",
        "if non_floats == 0:\n",
        "  print(\"All values in the dataset are floats\")\n",
        "else:\n",
        "  print(f\"{non_floats} values in dataset are not floats\")"
      ],
      "metadata": {
        "id": "4aVtVkv1dljh",
        "outputId": "0992259e-f3ce-40dc-dd8d-0744937edd17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All values in the dataset are floats\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check for duplicates**"
      ],
      "metadata": {
        "id": "OGrob7DIZl6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find duplicate columns and delete them\n",
        "def delete_duplicate_columns(arr):\n",
        "    num_cols = arr.shape[1]\n",
        "    duplicate_columns = []\n",
        "    for i in range(num_cols):\n",
        "        for j in range(i + 1, num_cols):\n",
        "            if np.array_equal(arr[:, i], arr[:, j]):\n",
        "                duplicate_columns.append(j)  # Append index of the duplicate column\n",
        "    if duplicate_columns:\n",
        "        # Delete duplicate columns\n",
        "        arr_without_duplicates = np.delete(arr, duplicate_columns, axis=1)\n",
        "        return arr_without_duplicates\n",
        "    else:\n",
        "        return arr\n",
        "\n",
        "# Remove duplicate columns\n",
        "X_train_clean = delete_duplicate_columns(X_train)\n",
        "\n",
        "print(\"Number of columns in original array:\",X_train.shape[1])\n",
        "print(\"Number of columns after removal of duplicate columns:\",X_train_clean.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyEMNXijZ37p",
        "outputId": "4d92073a-417c-4a98-b440-103e41bbfb04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of columns in original array: 493\n",
            "Number of columns after removal of duplicate columns: 465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking for outliers**\n",
        "Using the Z-score"
      ],
      "metadata": {
        "id": "6c9Rj_XInE6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_rows, num_cols = X_train_clean.shape\n",
        "total_outliers = 0\n",
        "\n",
        "for i in range(num_cols):\n",
        "    z = np.abs(stats.zscore(X_train_clean[:,i]))\n",
        "\n",
        "    threshold = 3\n",
        "    outliers = X_train_clean[z > threshold, i]\n",
        "    total_outliers +=(len(outliers))\n",
        "\n",
        "print(f'Total number of outliers  = {total_outliers}/{num_rows*num_cols}')\n",
        "print(f'Avarage number of outliers per feature = {round(total_outliers/num_cols,2)}/{num_rows}')\n"
      ],
      "metadata": {
        "id": "mPIPeNp-nJTv",
        "outputId": "36fdd3fe-f42c-42f6-d36c-0be0fba69dea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of outliers  = 661/42780\n",
            "Avarage number of outliers per feature = 1.42/92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Check if the data is normally distributed**\n",
        "Using the Kolmogorov Smirnov"
      ],
      "metadata": {
        "id": "u3RmZ5isz4Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_not_normal = 0\n",
        "p_threshold = 0.05\n",
        "\n",
        "for l in range(num_cols):\n",
        "    kstest_result = stats.kstest(X_train_clean[:,l], 'norm')\n",
        "    if kstest_result.pvalue < p_threshold:\n",
        "        total_not_normal += 1\n",
        "\n",
        "print(f'Total features that are not normally distributed = {total_not_normal}/{num_cols}')\n"
      ],
      "metadata": {
        "id": "PB_LKu7Bz3Ul",
        "outputId": "5da77fa1-0306-42d8-ec9f-273d018df4eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total features that are not normally distributed = 464/465\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scaling**\n",
        "Robust scaling because outliers and non Gaussian distribution"
      ],
      "metadata": {
        "id": "Ammb7jQ7RCPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale the data to be normal\n",
        "scaler = preprocessing.RobustScaler()\n",
        "scaler.fit(X_train_clean)\n",
        "X_train_scaled = scaler.transform(X_train_clean)"
      ],
      "metadata": {
        "id": "SjdjlpnSRGGd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature selection**\n",
        "Using L1 because it can better handle high dimensionality, it migigates overfitting and is easy to interpret"
      ],
      "metadata": {
        "id": "qNAjbYOLjx53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original data shape:\",X_train_scaled.shape)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "lsvc = LogisticRegression(C=0.1, penalty=\"l1\", dual=False, solver='liblinear').fit(X_train_scaled, y_train)\n",
        "model = SelectFromModel(lsvc, prefit=True)\n",
        "X_feature = model.transform(X_train_scaled)\n",
        "print(\"New data shape:\",X_feature.shape)"
      ],
      "metadata": {
        "id": "lws8rM7ojG3R",
        "outputId": "ea451e99-cac9-4af8-b70c-71bf39c6ac50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (92, 465)\n",
            "New data shape: (92, 23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PCA**"
      ],
      "metadata": {
        "id": "dm3msQI8WZ45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Voer PCA uit\n",
        "pca = decomposition.PCA(n_components=10)  #n_components is the amount of features that remain after PCA (can also be a ratio)\n",
        "X_pca = pca.fit_transform(X_feature)\n",
        "\n",
        "# Verklaarde variantie ratio's\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Cumulatieve verklaarde variantie\n",
        "cumulative_explained_variance = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "# Maak een DataFrame om de resultaten mooi weer te geven\n",
        "df = pd.DataFrame({\n",
        "    'Principal Component': [f'PC{i+1}' for i in range(len(explained_variance_ratio))],\n",
        "    'Explained Variance Ratio': explained_variance_ratio,\n",
        "    'Cumulative Explained Variance': cumulative_explained_variance\n",
        "})\n",
        "\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qm-dD7rWg_g",
        "outputId": "b698decd-b1d7-4287-96b2-b6039501374f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Principal Component  Explained Variance Ratio  Cumulative Explained Variance\n",
            "0                 PC1                  0.959157                       0.959157\n",
            "1                 PC2                  0.026176                       0.985333\n",
            "2                 PC3                  0.007162                       0.992495\n",
            "3                 PC4                  0.004295                       0.996790\n",
            "4                 PC5                  0.002573                       0.999364\n",
            "5                 PC6                  0.000305                       0.999668\n",
            "6                 PC7                  0.000203                       0.999871\n",
            "7                 PC8                  0.000058                       0.999929\n",
            "8                 PC9                  0.000041                       0.999970\n",
            "9                PC10                  0.000018                       0.999988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random forest**"
      ],
      "metadata": {
        "id": "ansRTksVbive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "scores_rfc = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in loo.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rfc.fit(X_train_fold,y_train_fold)\n",
        "    pred_rfc = rfc.predict(X_val_fold)\n",
        "    accuracy_rfc = accuracy_score(y_val_fold, pred_rfc)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_rfc.append(accuracy_rfc)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_rfc)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_rfc = sum(scores_rfc) / len(scores_rfc)\n",
        "print(\"Average Accuracy:\", average_accuracy_rfc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKeuhOtKbiFC",
        "outputId": "b828b819-6501-4851-810e-9d6b0727ba9b"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Average Accuracy: 0.6847826086956522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {'n_estimators': range(50, 200, 10)}  # Adjust the range as needed\n",
        "\n",
        "# Initialize Random Forest classifier\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the data\n",
        "grid_search.fit(X_feature, y_train)\n",
        "\n",
        "# Get the best estimator from grid search\n",
        "best_rfc = grid_search.best_estimator_\n",
        "\n",
        "# Get the best parameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best score from grid search\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ],
      "metadata": {
        "id": "eoPxSwDWyRBT",
        "outputId": "6ec49ac7-20f8-4c6f-d188-32a0a2a41eda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'n_estimators': 120}\n",
            "Best Score: 0.782608695652174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SVM**"
      ],
      "metadata": {
        "id": "M57SpGqZeDhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "scores_clf = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in loo.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    clf = svm.SVC()\n",
        "    clf.fit(X_train_fold,y_train_fold)\n",
        "    pred_clf = clf.predict(X_val_fold)\n",
        "    accuracy_clf = accuracy_score(y_val_fold, pred_clf)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_clf.append(accuracy_clf)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_clf)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_clf = sum(scores_clf) / len(scores_clf)\n",
        "print(\"Average Accuracy:\", average_accuracy_clf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rygV2hV-eJ37",
        "outputId": "ffc0fdaa-2047-4ca1-b32a-a69677e5779b"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0]\n",
            "Average Accuracy: 0.532608695652174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],                      # Regularization parameter\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],    # Kernel type\n",
        "    'gamma': [0.1, 1, 10],                  # Kernel coefficient (for 'poly' and 'rbf')\n",
        "    'degree': [2, 3, 4],                    # Degree of the polynomial kernel\n",
        "    'coef0': [0.0, 0.1, 1.0]                # Independent term in kernel function (for 'poly' and 'sigmoid')\n",
        "}\n",
        "\n",
        "# Initialize SVM classifier\n",
        "svc = svm.SVC()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=svc, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the data\n",
        "grid_search.fit(X_feature, y_train)\n",
        "\n",
        "# Get the best estimator from grid search\n",
        "best_svc = grid_search.best_estimator_\n",
        "\n",
        "# Get the best parameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best score from grid search\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ],
      "metadata": {
        "id": "SUec95H5227d",
        "outputId": "9e35978e-c6e6-4627-a995-6114165eb4f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
            "Best Score: 0.8260869565217391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural network**"
      ],
      "metadata": {
        "id": "SGCWQPk4fS1K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize KFold cross-validator\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "scores_mlpc = []\n",
        "# Iterate over each fold\n",
        "for train_index, val_index in loo.split(X_feature, y_train):\n",
        "    # Split data into train and validation sets for this fold\n",
        "    X_train_fold, X_val_fold = X_feature[train_index], X_feature[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "    mlpc = MLPClassifier(hidden_layer_sizes=(50,50),max_iter=500, random_state=42)\n",
        "    mlpc.fit(X_train_fold, y_train_fold)\n",
        "    pred_mlpc = mlpc.predict(X_val_fold)\n",
        "    accuracy_mlpc = accuracy_score(y_val_fold, pred_mlpc)\n",
        "\n",
        "    # Append accuracy to scores list\n",
        "    scores_mlpc.append(accuracy_mlpc)\n",
        "\n",
        "# Print the accuracy scores for each fold\n",
        "print(\"Accuracy scores for each fold:\", scores_mlpc)\n",
        "\n",
        "# Calculate and print the average accuracy across all folds\n",
        "average_accuracy_mlpc = sum(scores_mlpc) / len(scores_mlpc)\n",
        "print(\"Average Accuracy:\", average_accuracy_mlpc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4dputYVfSc9",
        "outputId": "b9a2ed5f-5654-4877-8e9f-073052e91312"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores for each fold: [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n",
            "Average Accuracy: 0.6630434782608695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],  # Number of neurons in each hidden layer\n",
        "    'activation': ['relu', 'tanh', 'logistic'],        # Activation function\n",
        "    'solver': ['adam', 'sgd', 'lbfgs'],                # Optimization algorithm\n",
        "    'alpha': [0.0001, 0.001, 0.01],                    # L2 regularization parameter\n",
        "    'learning_rate': ['constant', 'adaptive'],         # Learning rate schedule\n",
        "    'max_iter': [200, 300, 400],                       # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Initialize MLPClassifier\n",
        "mlp = MLPClassifier()\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=loo, scoring='accuracy')\n",
        "\n",
        "# Fit the GridSearchCV to the data\n",
        "grid_search.fit(X_feature, y_train)\n",
        "\n",
        "# Get the best estimator from grid search\n",
        "best_mlp = grid_search.best_estimator_\n",
        "\n",
        "# Get the best parameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Get the best score from grid search\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)"
      ],
      "metadata": {
        "id": "KaUKd5x1pSz1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}